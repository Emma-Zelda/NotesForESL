{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study Notes for *The Elements of Statistical Learning*\n",
    "\n",
    "### Edited by Emma Teng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2 Overview of Supervised Learning \n",
    "### 2.1 Introduction\n",
    "\n",
    "1. Termonologies for X and Y:\n",
    "    \n",
    "    - X: inputs, predictors, independent variables, features;\n",
    "    - Y: outputs, reponses, dependent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Variable Types and Terminology\n",
    "1. The distinction in output type has led to a naming convention for the prediction tasks:\n",
    "    \n",
    "    - regression: when we predict quantitative outputs;\n",
    "    - classification: when we predict qualitative outputs.\n",
    "    \n",
    "    \n",
    "2. Variable types include: quantitative, qualitative and ordered categorical.\n",
    "\n",
    "\n",
    "3. The most useful and commonly used coding for *qualitative* variables are **dummy variables**. K-level qualitative variable is represented by a vector of K binary variables or bits, only one of which is “on” at a time.\n",
    "\n",
    "\n",
    "4. Naming Convention: \n",
    "   - Inputs: X;\n",
    "   - Quantitative ouputs: Y;\n",
    "   - Qualitative outputs: G\n",
    "   - Use uppercase letters such as X, Y or G when referring to the generic aspects of a variable. Observed values are written in lowercase; hence the *i*th observed value of X is written as $x_i$ (where $x_i$ is again a scalar or vector).\n",
    "   - Matrices are represented by bold uppercase letters; for example, a set of N input p-vectors $x_i, i = 1, . . . ,N$ would be represented by the $N×p$ matrix X. \n",
    "   - The $i$th row of **X** is $x_i^T$, the vector transpose of $x_i$, because we assume all vectors are column vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Two Simple Approaches to Prediction: Least Squares and Nearest Neighbors\n",
    "#### 2.3.1 Linear Models and Least Square\n",
    "\n",
    "1. The linear model is:\n",
    "\\begin{align}\n",
    "\\hat{Y} & = \\hat{\\beta_0} + \\sum_{j=1}^p X_j \\hat{\\beta_j} \\\\\n",
    "& = X^T \\hat{\\beta}  \\\\\n",
    "\\end{align}\n",
    "where $X^T = (1, X_1, X_2, ..., X_p)$, $X$ is a $N\\times p$ matrix.\n",
    "\n",
    "\n",
    "2. Using Least Squares for finding unknown coefficients and the goal is to minimize the residual sum of squares:\n",
    "\\begin{align}\n",
    "RSS(\\beta) & = \\sum_{i = 1}^N (y_i - x_i^T\\beta)^2 \\\\\n",
    "&=(\\boldsymbol{y} - X\\beta)^T(\\boldsymbol{y}-X\\beta)\n",
    "\\end{align}\n",
    "where $X$ is an $N\\times p$ matrix with each row an input vector, and $\\boldsymbol{y}$ is an $N$-vector of the ouputs in the training set.\n",
    "\n",
    "    $RSS(\\beta)$ is a quadratic function of the parameters, and hence its minimum always exists, but may not be unique.\n",
    "\n",
    "    Differentiating w.r.t. $\\beta$ we get the normal equations:\n",
    "    \\begin{align}\n",
    "    X^T(\\boldsymbol{y} - X\\beta) = 0\n",
    "    \\end{align}\n",
    "\n",
    "    If $X^TX$ is nonsingular, then the unique solution is given by\n",
    "    \\begin{align}\n",
    "    \\hat{\\beta} = (X^T X)^{-1} X^T \\boldsymbol{y}\n",
    "    \\end{align}\n",
    "\n",
    "\n",
    "3. Using linear model for classification:\n",
    "    \n",
    "    First code all classes as a binary variable, and then fit by linear regression. The decision boundary can be $x^T \\hat{\\beta} = 0.5$.\n",
    "\n",
    "\n",
    "4. Two possible scenarios regarding to Linear Regression Classification performance:\n",
    "\n",
    "    <span style=\"color:blue\">Scenario 1:</span> The training data in each class were generated from bivariate Gaussian distributions with uncorrelated components and different means.\n",
    "    \n",
    "    <span style=\"color:blue\">Scenario 2:</span> The training data in each class came from a mixture of 10 low-variance Gaussian distributions, with individual means themselves distributed as Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
